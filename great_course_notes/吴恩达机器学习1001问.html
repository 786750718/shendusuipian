<h1>吴恩达机器学习：一千零一问</h1>
<p><a href="https://github.com/EmbraceLife/shendusuipian/tree/master/great_course_notes">获取最新PDF版本</a></p>
<p><a href="https://study.163.com/course/courseMain.htm?courseId=1004570029">吴恩达机器学习 - 网易云课堂</a></p>
<p><a href="https://www.bilibili.com/video/av9912938/?spm_id_from=333.788.videocard.1">吴恩达机器学习 中英字幕B站版本</a></p>
<p><a href="http://link.zhihu.com/?target=https%3A//www.bilibili.com/video/av20994456/">第一版笔记视频专辑 2018.4.29完结</a></p>
<p><a href="https://zhuanlan.zhihu.com/c_183710782">探索一句话版的机器学习和深度学习</a></p>
<p><a href="bear://x-callback-url/open-note?title=Becoming%20a%20macro%20investor&x-error=bear%3A%2F%2Fx%2Dcallback%2Durl%2Fcreate%3Ftitle%3DBecoming%2520a%2520macro%2520investor">Becoming a macro investor</a></p>
<br>
<p><mark>用50个小问题梳理吴恩达机器学习课程的直觉认知</mark></p>
<p>我的回答搜录在<a href="https://www.bilibili.com/video/av39711506/">图解吴恩达机器学习（2019直觉版）持续更新</a></p>
<p><a href="https://www.bilibili.com/video/av39711506/?p=1">制作直觉版视频的初衷</a></p>
<br>
<p><mark>用数百个小问题总结吴恩达机器学习课程内核心视频知识点</mark></p>
<p>所有的回答，通过原视频剪辑方式，<a href="https://www.bilibili.com/video/av40544023/">让视频片段自己回答问题</a></p>
<br>
<h1>第一课 初步认识机器学习 Introduction</h1>
<p><mark>你是如何定义机器学习的？</mark></p>
<p><mark>在你脑海里机器学习的流程/模块链长什么样子？</mark></p>
<p><mark>基于你的模块链，如何定义和区分监督和无监督学习？</mark></p>
<p>1 - 1 - Welcome (7 min) 课程介绍</p>
<p>1 - 2 - What is Machine Learning 什么是机器学习 (7 min)</p>
<p>1 - 3 - Supervised Learning  监督学习(12 min)</p>
<p>1 - 4 - Unsupervised Learning 无监督学习 (14 min) </p>
<br>
<p><a href="bear://x-callback-url/open-note?title=What%20is%20machine%20learning%20Notes&x-error=bear%3A%2F%2Fx%2Dcallback%2Durl%2Fcreate%3Ftitle%3DWhat%2520is%2520machine%2520learning%2520Notes">What is machine learning Notes</a></p>
<br>
<h1>第二课 单变量线性回归 Univariate Linear Regression</h1>
<p><mark>基于ML模块链，监督学习与线性回归的异同，如何理解？</mark> y</p>
<p><mark>线性回归模型要解决什么问题？</mark> 近似y</p>
<p><mark>线性回归模型的目标函数，你要怎么设计？</mark> 到y的距离</p>
<p><mark>目标函数本身到底在做什么？存在的意义是什么？</mark> 评价</p>
<p><mark>目标函数的目的又是什么？追寻极小（大）值的目的是什么？</mark>更新</p>
<p><mark>为什么我们需要更智慧的搜索更新参数的方法？逐一或随机搜索的问题在哪？</mark></p>
<p><mark>梯度下降搜索的智慧之处如何体现？</mark></p>
<p><mark>梯度下降，是如何做到识别参数更新方向，以及损失值距离最小值的远近程度？</mark></p>
<p><mark>为什么这里是指损失值距离最小值远近程度，而非参数距离使损失值最小化状态位置的远近程度？</mark></p>
<br>
<p>2 - 1 - Model Representation 模型描述 (8 min)</p>
<p>2 - 2 - Cost Function 代价函数 (8 min)</p>
<p>2 - 3 - Cost Function - Intuition I 代价函数 - 直觉 1 (11 min)</p>
<p>2 - 4 - Cost Function - Intuition II 代价函数 - 直觉 2 (9 min)</p>
<p>2 - 5 - Gradient Descent 梯度下降 (11 min)</p>
<p>2 - 6 - Gradient Descent Intuition 梯度下降的直觉 (12 min)</p>
<p>2 - 7 - Gradient Descent For Linear Regression 线性回归的梯度下降 (6 min)</p>
<p>2 - 8 - What_'s Next 总结 (6 min)</p>
<p><a href="bear://x-callback-url/open-note?title=Univariate%20Linear%20Regression%20Detailed%20Notes&x-error=bear%3A%2F%2Fx%2Dcallback%2Durl%2Fcreate%3Ftitle%3DUnivariate%2520Linear%2520Regression%2520Detailed%2520Notes">Univariate Linear Regression Detailed Notes</a></p>
<br>
<h1>第三课 线性代数回顾 Linear Algebra Review</h1>
<p><mark>基本入门线性代数帮助我们加速计算</mark></p>
<br>
<p>3 - 1 - Matrices and Vectors 矩阵和向量 (9 min)</p>
<p>3 - 2 - Addition and Scalar Multiplication 加法和标量乘法 (7 min)</p>
<p>3 - 3 - Matrix Vector Multiplication 矩阵向量乘法 (14 min)</p>
<p>3 - 4 - Matrix Matrix Multiplication 矩阵乘法 (11 min)</p>
<p>3 - 5 - Matrix Multiplication Properties 矩阵乘法特征属性 (9 min)</p>
<p>3 - 6 - Inverse and Transpose 逆和转置 (11 min)</p>
<br>
<p><a href="bear://x-callback-url/open-note?title=%E5%90%B4%E6%81%A9%E8%BE%BE%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E6%89%80%E9%9C%80%E7%9A%84%E6%89%80%E6%9C%89%E7%BA%BF%E6%80%A7%E4%BB%A3%E6%95%B0&x-error=bear%3A%2F%2Fx%2Dcallback%2Durl%2Fcreate%3Ftitle%3D%25E5%2590%25B4%25E6%2581%25A9%25E8%25BE%25BE%25E6%259C%25BA%25E5%2599%25A8%25E5%25AD%25A6%25E4%25B9%25A0%25E6%2589%2580%25E9%259C%2580%25E7%259A%2584%25E6%2589%2580%25E6%259C%2589%25E7%25BA%25BF%25E6%2580%25A7%25E4%25BB%25A3%25E6%2595%25B0">吴恩达机器学习所需的所有线性代数</a></p>
<br>
<br>
<h1>第四课多变量线性回归 Multivariate Linear Regression</h1>
<p><mark>为什么多特征多变量才是我们面对的更现实的世界？</mark></p>
<p><mark>多参数更新公式与单参数更新公式有多少差异？</mark></p>
<p><mark>特征放缩是怎样的处理？</mark></p>
<p><mark>怎样的现象和问题导致特征放缩成为必要的数据处理技巧？</mark></p>
<p><mark>为什么要设计学习率的存在？可以解决怎样的问题？（对应上一课的最后的问题）</mark></p>
<p><mark>面对复杂的数据（比线性更复杂），为什么升级成高阶的多项式能建复杂模型解决问题？</mark></p>
<p><mark>为什么迭代方法比分析求解更具普世务实意义？</mark></p>
<br>
<p>4 - 1 - Multiple Features 多特征值 多变量(8 min)</p>
<p>4 - 2 - Gradient Descent for Multiple Variables 多变量梯度下降法 (5 min)</p>
<p>4 - 3 - Gradient Descent in Practice I - Feature Scaling  多变量梯度下降演练法-特征放缩</p>
<p>4 - 4 - Gradient Descent in Practice II - Learning Rate 多变量梯度下降演练法-学习率</p>
<p>4 - 5 - Features and Polynomial Regression 特征与多项式回归</p>
<p>4 - 6 - Normal Equation 正规方程（区别于迭代方法的直接解法）</p>
<p>4 - 7 - Normal Equation Noninvertibility 正规方程在矩阵不可逆情况下的解决方法</p>
<br>
<p><a href="bear://x-callback-url/open-note?title=multivariate%20linear%20regression%20detailed%20notes&x-error=bear%3A%2F%2Fx%2Dcallback%2Durl%2Fcreate%3Ftitle%3Dmultivariate%2520linear%2520regression%2520detailed%2520notes">multivariate linear regression detailed notes</a></p>
<br>
<h1>第五章 Octave 教程</h1>
<p>5 - 1 - Basic Operations 基本操作</p>
<p>5 - 2 - Moving Data Around 移动数据</p>
<p>5 - 3 - Computing on Data 计算数据</p>
<p>5 - 4 - Plotting Data 数据绘制</p>
<p>5 - 5 - Control Statements_ for, while, if statements 控制语句：for，while，if 语句</p>
<p>5 - 6 - Vectorization 矢量</p>
<p>5 - 7 - Working on and Submitting Programming Exercises </p>
<p><a href="bear://x-callback-url/open-note?title=Octave%20Notes&x-error=bear%3A%2F%2Fx%2Dcallback%2Durl%2Fcreate%3Ftitle%3DOctave%2520Notes">Octave Notes</a></p>
<br>
<h1>第六课 Logistic Regression 逻辑回归</h1>
<p><mark>如何理解分类模型与监督学习以及线性回归之间的关系？</mark></p>
<p><mark>为什么线性回归不能解决分类问题？从数据角度，怎么理解？</mark></p>
<p><mark>为什么要采纳sigmoid激活函数处理最后输出值？为什么不可以直接将所有值都压缩到0-1，不是更简单吗？</mark></p>
<p><mark>为什么不继续沿用MSE，用预测值和真实值之间的距离远近来评估好坏？而采用logloss的新方法？</mark></p>
<p><mark>多元分类问题，能不能用二元分类来解决？具体应该如何操作才能化多元问题为多个二元模型来处理？</mark></p>
<p><mark>有没有不需要调试学习率的优化算法，而且比GD效果更好？这些高级算法的弊端是什么呢？</mark></p>
<br>
<p>6 - 1 - Classification 分类</p>
<p>6 - 2 - Hypothesis Representation <s>假设陈述</s> 设计模型</p>
<p>6 - 3 - Decision Boundary 决策界限</p>
<p>6 - 4 - Cost Function 代价函数</p>
<p>6 - 5 - Simplified Cost Function and Gradient Descent 简化代价函数与梯度下降</p>
<p>6 - 6 - Advanced Optimization 高级优化</p>
<p>6 - 7 - Multiclass Classification_ One-vs-all 多元分类：一对多</p>
<br>
<p><a href="bear://x-callback-url/open-note?title=Logistic%20Regression%20Detailed%20Notes&x-error=bear%3A%2F%2Fx%2Dcallback%2Durl%2Fcreate%3Ftitle%3DLogistic%2520Regression%2520Detailed%2520Notes">Logistic Regression Detailed Notes</a></p>
<br>
<br>
<br>
<h1>第七课 正则化 Regularization</h1>
<p><mark>为什么模型容易出现过拟合？更多更高阶的特征能画更复杂模型拟合曲线，但多少特征，多高阶层，能够合适恰当？容易过多过高</mark></p>
<p><mark>过拟合的模型有怎样的症状？对比训练和测试效果</mark></p>
<p><mark>为什么会有如此症状？根源逻辑是什么？搜索空间大和案例少角度</mark></p>
<p><mark>如何缓解甚至治疗过拟合？相同角度所对应的方法</mark></p>
<p><mark>为什么说正则化，是变相间接减少特征值？</mark></p>
<br>
<p>7 - 1 - The Problem of Overfitting 过拟合问题</p>
<p>7 - 2 - Cost Function 代价函数</p>
<p>7 - 3 - Regularized Linear Regression 线性回归的正则化</p>
<p>7 - 4 - Regularized Logistic Regression 逻辑回归的正则化</p>
<br>
<p><a href="bear://x-callback-url/open-note?title=Regularization%20Detailed%20Notes&x-error=bear%3A%2F%2Fx%2Dcallback%2Durl%2Fcreate%3Ftitle%3DRegularization%2520Detailed%2520Notes">Regularization Detailed Notes</a></p>
<br>
<h1>第八课 神经网络学习 Neural Net</h1>
<p><mark>为什需要神经网络？预测值和分类没问题，过拟合也没问题，为什么需要算法？更优的特征选择或创造角度</mark></p>
<p><mark>多项式选择和创造特征上存在怎样的问题？</mark></p>
<p><mark>神经网络在创造和选择特征上如何高明？数据控制，创造灵活和信息全面度与深度的角度</mark></p>
<p><mark>神经网络的本质的简单理解？从逻辑回归角度</mark></p>
<p><mark>从机器学习模块链，如何梳理神经网络训练流程？</mark></p>
<br>
<p>8 - 1 - Non-linear Hypotheses 非线性假设</p>
<p>8 - 2 - Neurons and the Brain 神经元与大脑</p>
<p>8 - 3 - Model Representation I 模型展示Ⅰ</p>
<p>8 - 4 - Model Representation II 模型展示ⅠI </p>
<p>8 - 5 - Examples and Intuitions I 例子与直觉理解Ⅰ</p>
<p>8 - 6 - Examples and Intuitions II 例子与直觉理解ⅠI</p>
<p>8 - 7 - Multiclass Classification 多元分类</p>
<p><a href="bear://x-callback-url/open-note?title=Neural%20Net%20Detailed%20Notes&x-error=bear%3A%2F%2Fx%2Dcallback%2Durl%2Fcreate%3Ftitle%3DNeural%2520Net%2520Detailed%2520Notes">Neural Net Detailed Notes</a></p>
<br>
<h1>第九课 神经网络参数的反向传播算法 Backpropagation</h1>
<p><mark>神经网络在模型更新与参数求导过程中有什么难点？如何解决的？</mark></p>
<p>9 - 1 - Cost Function 代价函数</p>
<p>9 - 2 - Backpropagation Algorithm 反向传播算法</p>
<p>9 - 3 - Backpropagation Intuition 理解反向传播</p>
<p>9 - 4 - Implementation Note_ Unrolling Parameters 使用注意：展开参数</p>
<p>9 - 5 - Gradient Checking 梯度检测</p>
<p>9 - 6 - Random Initialization 随机初始化</p>
<p>9 - 7 - Putting It Together 组合到一起</p>
<p>9 - 8 - Autonomous Driving 无人驾驶</p>
<p><a href="bear://x-callback-url/open-note?title=Backpropagation%20Detailed%20Notes&x-error=bear%3A%2F%2Fx%2Dcallback%2Durl%2Fcreate%3Ftitle%3DBackpropagation%2520Detailed%2520Notes">Backpropagation Detailed Notes</a></p>
<br>
<h1>第十课 应用机器学习的建议 Practical Advice on ML</h1>
<p><mark>有应对过拟合与欠拟合的方法，但我们能在训练过程中及时准确识别模型现在所处的是哪种状态吗？</mark></p>
<p><mark>为什么画出训练损失值与训练集训练次数的关系图，我们无法判断模型到底是过拟合还是欠拟合？</mark></p>
<p><mark>为什么有了训练与测试损失值的对比后的关系图，就可以判断是否过拟合或者欠拟合？</mark></p>
<p><mark>为什么光有测试集，没有验证集还不够，反复使用测试集实验超参数，会出现怎样的问题？</mark></p>
<p><mark>增加了验证集，为什么就能解决选择超参数相关的问题？</mark></p>
<p><mark>我们有三张图可以帮助判断模型的拟合状态，分别是哪三张图？他们的工作原理是什么？提示：（训练+验证，iters）（训练+验证，data size），（训练+验证，lambda）</mark></p>
<p><mark>过拟合，欠拟合，OK在图中会以怎样的形态出现？</mark></p>
<br>
<p>如果只有训练集</p>
<p>10 - 1 - Deciding What to Try Next 决定下一步做什么</p>
<p>10 - 2 - Evaluating a Hypothesis 评估假设</p>
<p>10 - 3 - Model Selection and Train<i>Validation</i>Test Sets 模型选择和训练、验证、测试集</p>
<p>10 - 4 - Diagnosing Bias vs. Variance 诊断偏差与方差</p>
<p>10 - 5 - Regularization and Bias_Variance 正则化和偏差、方差</p>
<p>10 - 6 - Learning Curves 学习曲线</p>
<p>10 - 7 - Deciding What to Do Next Revisited 决定接下来做什么</p>
<p><a href="bear://x-callback-url/open-note?title=Practical%20Advice%20on%20ML%20Detailed%20Notes&x-error=bear%3A%2F%2Fx%2Dcallback%2Durl%2Fcreate%3Ftitle%3DPractical%2520Advice%2520on%2520ML%2520Detailed%2520Notes">Practical Advice on ML Detailed Notes</a></p>
<br>
<br>
<br>
<h1>第十一课 机器学习系统设计 Design ML System</h1>
<p><mark>我们有了机器学习的模块链，知道构建机器学习系统需要的核心步骤，但是具体实操的优良经验是需要积累和传承的。吴恩达传承给我们的设计构建模型的实操经验是怎样的呢？</mark></p>
<p><mark>从错误中学习成长，成长多少是需要量化度量的，我们该使用怎样的指标呢？</mark></p>
<p><mark>为什么错误率，面对skewed数据时，往往无效？而精确度与召回率却是更好的选择，为什么？</mark></p>
<p><mark>为什么使用精确度和召回率很费事？首先，精确度和召回率到底是什么，如何定义的？为什么他们之间存在博弈权衡（此消彼长）关系？为什么在现实世界中，我们必须要权衡他们的大小关系？</mark></p>
<p><mark>F1 score是如何融合精确度与召回率，让我们轻松无脑实现两者的合理高效的平衡关系？</mark></p>
<br>
<br>
<p>11 - 1 - Prioritizing What to Work On 确定执行的优先级</p>
<p>11 - 2 - Error Analysis 误差分析</p>
<p>11 - 3 - Error Metrics for Skewed Classes 不对称性分类的误差评估</p>
<p>11 - 4 - Trading Off Precision and Recall 精确度和召回率的权衡</p>
<p>11 - 5 - Data For Machine Learning 机器学习数据</p>
<p><a href="bear://x-callback-url/open-note?title=Design%20ML%20System%20Detailed%20Note&x-error=bear%3A%2F%2Fx%2Dcallback%2Durl%2Fcreate%3Ftitle%3DDesign%2520ML%2520System%2520Detailed%2520Note">Design ML System Detailed Note</a></p>
<br>
<br>
<br>
<br>
<h1>第十二课 支持向量机 Support Vector Machine</h1>
<p><mark>已经有了线性回归，逻辑回归，神经网络，为什么要学习SVM？</mark></p>
<p><mark>SVM如何做到更简单干净的？对比逻辑回归的损失函数</mark></p>
<p><mark>为什么说SVM比逻辑回归，线性回归更强大？从创造特征角度</mark></p>
<br>
<p>12 - 1 - Optimization Objective 优化目标</p>
<p>12 - 2 - Large Margin Intuition 直观上对大间隔的理解</p>
<p>12 - 3 - Mathematics Behind Large Margin Classification 大间隔分类器的数学原理</p>
<p>12 - 4 - Kernels I 核函数 I </p>
<p>12 - 5 - Kernels II 核函数 II</p>
<p>12 - 6 - Using An SVM 使用SVM </p>
<br>
<p><a href="bear://x-callback-url/open-note?title=SVM%20Detailed%20Notes&x-error=bear%3A%2F%2Fx%2Dcallback%2Durl%2Fcreate%3Ftitle%3DSVM%2520Detailed%2520Notes">SVM Detailed Notes</a></p>
<br>
<br>
<h1>第十三课 无监督学习 Unsupervised Learning</h1>
<p><mark>无监督学习的无监督体现在哪里？</mark></p>
<p><mark>为什么说无监督学习其实还是有监督的机制在工作？</mark></p>
<p><mark>为什么说聚类其实就是无监督学习的分类？</mark></p>
<p><mark>为什么说无监督学习，其实也是有目标值的，怎么理解？</mark></p>
<p><mark>K-means的工作原理是什么？</mark></p>
<p><mark>如何理解K-means中假设出来的目标值与生成的预测值？</mark></p>
<p><mark>如何理解K-means的目标函数或损失函数？</mark></p>
<p><mark>为什么说K-means自带优化算法？</mark></p>
<br>
<p>13 - 1 - Unsupervised Learning_ Introduction 无监督学习简介</p>
<p>13 - 2 - K-Means Algorithm K-Means算法</p>
<p>13 - 3 - Optimization Objective 优化目标</p>
<p>13 - 4 - Random Initialization 随机初始化</p>
<p>13 - 5 - Choosing the Number of Clusters 选取聚类数量</p>
<p><a href="bear://x-callback-url/open-note?title=Unsupervised%20Learning%20Detailed%20Notes&x-error=bear%3A%2F%2Fx%2Dcallback%2Durl%2Fcreate%3Ftitle%3DUnsupervised%2520Learning%2520Detailed%2520Notes">Unsupervised Learning Detailed Notes</a></p>
<br>
<br>
<br>
<h1>第十四课 降维 Dimensionality Reduction</h1>
<p><mark>如何理解无监督学习中的维度下降的本质含义？</mark></p>
<p><mark>维度下降算法的常见的实际用途是什么？</mark></p>
<p><mark>维度下降算法的工作原理是什么？</mark></p>
<p><mark>维度下降算法的工作难点在哪里？</mark></p>
<p><mark>维度下降算法的目标函数如何理解？</mark></p>
<p><mark>维度下降如何优化搜索最优空间维度数？</mark></p>
<br>
<p>14 - 1 - Motivation I_ Data Compression 目标 I：数据压缩</p>
<p>14 - 2 - Motivation II_ Visualization 目标 II：可视化</p>
<p>14 - 3 - Principal Component Analysis Problem Formulation 主成分分析问题规划</p>
<p>14 - 4 - Principal Component Analysis Algorithm 主成分分析算法</p>
<p>14 - 5 - Choosing the Number of Principal Components 主成分数量选择</p>
<p>14 - 6 - Reconstruction from Compressed Representation 压缩 <s>重现</s>还原</p>
<p>14 - 7 - Advice for Applying PCA 应用 PCA 的建议</p>
<p><a href="bear://x-callback-url/open-note?title=Dimensionality%20Reduction%20Detailed%20Notes&x-error=bear%3A%2F%2Fx%2Dcallback%2Durl%2Fcreate%3Ftitle%3DDimensionality%2520Reduction%2520Detailed%2520Notes">Dimensionality Reduction Detailed Notes</a></p>
<br>
<br>
<br>
<br>
<h1>第十五课 异常检测 Anomaly Detection</h1>
<p><mark>异常检测的本质是要解决什么问题？</mark></p>
<p><mark>为什么异常检测的问题无法用监督学习的分类来解决？</mark></p>
<p><mark>为什么在依赖数据特征值，像无监督学习一样，尝试做聚类（分类）时，又引入目标值（正常VS异常）来帮助训练模型？</mark></p>
<p><mark>异常检测，是如何利用概率分布，解决聚类或分类问题的？</mark></p>
<p><mark>异常检测的目标函数与优化算法长什么样子？</mark></p>
<br>
<p>15 - 1 - Problem Motivation 问题动机</p>
<p>15 - 2 - Gaussian Distribution 高斯分布</p>
<p>15 - 3 - Algorithm 算法</p>
<p>15 - 4 - Developing and Evaluating an Anomaly Detection System 开发和评估异常检测系统</p>
<p>15 - 5 - Anomaly Detection vs. Supervised Learning 异常检测 VS 监督学习</p>
<p>15 - 6 - Choosing What Features to Use 选择要使用的特征</p>
<p>15 - 7 - Multivariate Gaussian Distribution 多变量高斯分布</p>
<p>15 - 8 - Anomaly Detection using the Multivariate Gaussian Distribution 使用多变量高斯分布的异常检测</p>
<br>
<p><a href="bear://x-callback-url/open-note?title=Anomaly%20Detection%20Detailed%20Notes&x-error=bear%3A%2F%2Fx%2Dcallback%2Durl%2Fcreate%3Ftitle%3DAnomaly%2520Detection%2520Detailed%2520Notes">Anomaly Detection Detailed Notes</a></p>
<br>
<br>
<br>
<h1>第十六课 推荐系统 Recommendation system</h1>
<p><mark>推荐系统，本质上是怎样的一种学习算法？</mark></p>
<p><mark>推荐系统的目标值，预测值，特征值，应该如何理解？</mark></p>
<p><mark>如何理解推荐系统是一个监督学习系统，有两组交替训练的参数？</mark></p>
<br>
<br>
<p>16 - 1 - Problem Formulation 问题规划</p>
<p>16 - 2 - Content Based Recommendations 基于内容的推荐算法</p>
<p>16 - 3 - Collaborative Filtering 协同过滤</p>
<p>16 - 4 - Collaborative Filtering Algorithm 协同过滤算法</p>
<p>16 - 5 - Vectorization_ Low Rank Matrix Factorization 矢量化：低轶矩阵分解</p>
<p>16 - 6 - Implementational Detail_ Mean Normalization 实施细节：均值规范化</p>
<p><a href="bear://x-callback-url/open-note?title=Recommendation%20system%20Detailed%20Notes&x-error=bear%3A%2F%2Fx%2Dcallback%2Durl%2Fcreate%3Ftitle%3DRecommendation%2520system%2520Detailed%2520Notes">Recommendation system Detailed Notes</a></p>
<br>
<br>
<br>
<br>
<h1>第十七课 大规模机器学习 large scale ML</h1>
<p><mark>数据量大，能帮助避免过拟合，产生更强大的模型；但从实操经验上看，先可以用小量数据感受模型改进方向</mark></p>
<p><mark>Batch GD 每次训练都拿出全部数据，结果精确稳定但效率低下</mark></p>
<p><mark>Stochastic GD每次只用一个数据，结果不精确但可接受，亮点是效率极高，全套数据训练一次，就能比较接近极小值convergence</mark></p>
<p><mark>mini-batch GD 每次拿一小组数据，结果比Batch差点但比较准确，同时可用向量加速计算，数据使用效率很高</mark></p>
<p><mark>如果数据量巨大且持续增加，没必要重复使用数据训练，可持续用新生成的数据训练模型</mark></p>
<p><mark>数据大到单台（或单核）计算机无法处理时，将数据分散到多台（或多核）计算机中同时计算，提升计算效率</mark></p>
<br>
<p>17 - 1 - Learning With Large Datasets 学习大数据集</p>
<p>17 - 2 - Stochastic Gradient Descent 随机梯度下降</p>
<p>17 - 3 - Mini-Batch Gradient Descent Mini-Batch 梯度下降</p>
<p>17 - 4 - Stochastic Gradient Descent Convergence 随机梯度下降收敛</p>
<p>17 - 5 - Online Learning 在线学习</p>
<p>17 - 6 - Map Reduce and Data Parallelism 减少映射与数据并行</p>
<br>
<p><a href="bear://x-callback-url/open-note?title=Large%20Scale%20ML%20Detailed%20Notes&x-error=bear%3A%2F%2Fx%2Dcallback%2Durl%2Fcreate%3Ftitle%3DLarge%2520Scale%2520ML%2520Detailed%2520Notes">Large Scale ML Detailed Notes</a></p>
<br>
<h1>第十八课 应用举例：照片OCR（光学字符识别）</h1>
<p><mark>真实机器学习项目往往需要分解成多个任务，需要多个机器学习模块串联起来，一步一步解决问题</mark></p>
<p>18 - 1 - Problem Description and Pipeline 问题描述与 OCR pipeline</p>
<p>18 - 2 - Sliding Windows 滑动窗口</p>
<p>18 - 3 - Getting Lots of Data and Artificial Data 获取大量数据和人工数据</p>
<p>18 - 4 - What Part of the Pipeline to Work on Next  获取大量数据和人工数据</p>
<br>
<p>19 - 1 - Summary and Thank You </p>
<br>
<p><a href="bear://x-callback-url/open-note?title=%E5%AE%9E%E4%BE%8B%E5%BA%94%E7%94%A8&x-error=bear%3A%2F%2Fx%2Dcallback%2Durl%2Fcreate%3Ftitle%3D%25E5%25AE%259E%25E4%25BE%258B%25E5%25BA%2594%25E7%2594%25A8">实例应用</a></p>
<br>
<br>
<br>
<br>
<br>
<br>
<br>
<br>
<br>
