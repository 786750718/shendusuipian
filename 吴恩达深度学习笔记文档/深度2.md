# 吴恩达深度学习课程2：实战中的问题

<!--改善深层神经网络：超参数调试、正则化以及优化-->

## 学习目标

- 不同的初始化，产生不一样的效果

- 初始化，对复杂的模型，意义重大

- train set, dev set, test set 之间的差异

- Diagnose the bias and variance issues in your model

- Learn when and how to use regularization methods such as dropout or L2 - regularization.

- Understand experimental issues in deep learning such as Vanishing or Exploding gradients and learn how to deal with them

- Use gradient checking to verify the correctness of your backpropagation implementation

- 理解业界构建深度神经网络应用最有效的做法。

  \- 能够高效地使用神经网络通用的技巧，包括初始化、L2和dropout正则化、Batch归一化、梯度检验。

  \- 能够实现并应用各种优化算法，例如mini-batch、Momentum、RMSprop和Adam，并检查它们的收敛程度。

  \- 理解深度学习时代关于如何构建训练/开发/测试集以及偏差/方差分析最新最有效的方法。

  \- 能够用TensorFlow实现一个神经网络。

# Week1 

# 构建你的机器学习应用

<!--Setting up your Machine Learning Application-->

## 理解Train | Dev | Test sets

<!--train dev test sets-->

![][2.1.01]
- 深度学习，重在循环实验，一步步推算出最优高阶参数

- 一个领域优秀模型的高阶参数，通常不能直接用于另一个领域

- 要高效实验出优秀的高阶参数，数据的量与维度，computation configuration 计算能力特点，都很重要


![][2.1.02]
- 数据分割大原则
  - 小数据（100） `0.6 | 0.2 | 0.2`
  - 大数据（1000000） `0.99 | 0.005 | 0.005`
- 为什么差异如此大：
  - 不变：`dev|valid sets` 用来比较高阶参数取什么值更优
  - 因此，本质上dev sets数据需求量并不大
  - 对于大数据，是需要拿出一点点比例就够了
- 实践意义：
  - 也许可以将90%以上的数据都留给`train set`

![][2.1.03]
- 很多时候`train set | dev | test sets` 有不同的`distributions`
  - 例如在猫的图片识别中：
    - train set of cat pics: 高清图片
    - dev（即valid) and test pics: 低分辨率图片
    - 所以，train vs dev sets 数据分布不同
- 以上这一点难避免，但有一点要保障：
  - `dev` and `test` sets 数据分布要相近
- `test set` 不是非要不可
  - `test set` 目的：实现unbias（无偏见，公正的）模型评估
  - 但这本身不是强制和必须的；为什么呢？
  - 因为毕竟`dev` and `test` sets 的分类是相似的
- 实际指导意义：
  - 我们不是一定需要`test set`, 可以留更多数据给`train set`
  - 当`dev set`和`test set`的数据占比很小，而且紧挨着，两者的分布应该不会相差太远

------

## ==Bias 与 Variance的关系== 

<!--Bias and Variance Trade off-->

### 主要规律与噪音的博弈度量

![][2.1.04]
- 左图: 函数线很粗旷，不少数据预测错误 （underfitting = highly biased)
- 右图：函数线过度精细，所有数据预测正确 (overfitting = highly variance)，预测效果会变差
- 中间：函数线平稳，有少数数据预测错误 （just right = low biased, low variance)

### 猫狗识别中的bias, variance, bayes error

![][2.1.05]

- **bayes error** :
  - 我们可以将人类识别误差标准作为基准，可以是0，也可能是15%（更高，如果分辨率低图片）
  - 如果基准误差是 15%，那么15%就不再是high bias

### 特例 **high bias & varance**

![][2.1.06]



------

## ==如何突破Bias-Variance困境== 

<!--Basic Recipe for Machine Learning-->

![][2.1.07]

### 降低high bias

- 在训练中，首先观察`high bias`，如果存在, 调试一下高阶参数
  - 构建更大模型，抓取更多更细特征
    - 增加隐藏层数
    - 增加每层神经元个数
  - 增加训练次数：
    - 可能有些特征，"埋藏"较深
  - 尝试不同 `NN architecture search`：
    - <!--没有详述，所以猜测内容如下-->
    - 神经层构造设计
      - 激励函数设计
    - 损失函数设计
    - 学习步伐大小
- 不断训练，优化上述参数，直到 `high bias` 解决，

### 再降低high variance

- 然后观察`high variance`,如果存在, 尝试以下措施
  - 增加数据量
    - 可能验证数据分布与训练数据差距过大
  - `regularization` 正则化处理
  - 尝试不同的神经层设计`NN architecture search`
- 不断训练，优化上述参数，直到`high variance`解决
- 需要不断循环上述流程，实现`low bias, low variance`

### 深度学习的独特优势

- `bias and variance tradeoff`难题 让deep learning 更受欢迎
  - 传统机器学习，`bias and variance` 是矛盾体， 很难实现双降双`low`
  - 深度学习, 只要有`bigger network`，和`more data`，双降是可实现的
  - `regularization`可以解决隐藏层过深而导致`high variance`的问题

------

# 规范你的神经网络

<!--Regularizing your neural network-->

## 何时用regularization正则化

<!--regularization-->

- 模型处于`overfitting`
  - 最重要的是， `low bias`，模型的训练效果大幅强于基准效果
  - `high variance`，验证效果距离训练效果差距明显
- 病根
  - 除了主要特征规律，该模型抓取了不少噪音，当作规律
  - 这些噪音，不可能在验证数据中具备预测能力，所以验证效果差
- regularization正则化意义
  - 阻止模型学习噪音
  - 让主要特征显性，让噪音特征更隐性

### `L2, L1` 正则化如何降噪

- `L2, L1`通过`lambd`控制`w`值的大小，从而控制模型的偏执
  - 模型偏执小了，就没有能力抓取过多的噪音，当作规律存储

![][2.1.08]
- Cost function 最小化： $min_{w,b}J(w,b), w\in{R^{n_x}}, b\in{R}$
- $J(w,b) = \frac{1}{m}\sum_{i=1}^mL(\hat{y^{(i)}},y^{(i)}) + \frac{\lambda}{2m}{||w||}_2^2$
- `L2`: ${||w||}_2^2 = \sum_{j=1}^{n_x}w_j^2 = w^Tw$
- `L1`: $\frac{\lambda}{2m}{||w||}_1 = \frac{\lambda}{2m}\sum_{j=1}^{n_x}|w_j|$
- 对于`L1`, `w` will be sparse：很多0构成
  - 一种观点：让许多`w`值归0，从而缩小模型
  - 但实际增效不大
  - 所以，实操中，`L2`用的更多


- $lambda$: regularization parameter
  - 帮助降低`high variance`的高阶参数
  - python 编程中，不要用`lambda`(key word), 而要用`lambd`

### `L2, L1` 在神经网络的用法

- ![][2.1.09]
  - 加入`L2`之后的`Cost function`的公式
    - $J(w^{[1]}, b^{[1]}, ..., w^{[L]}, b^{[L]})$
    - $=\frac{1}{m}\sum_{i=1}^mL({\hat{y}}^{(i)}, y^{(i)})+\frac{\lambda}{2m}\sum_{l=1}^L{||w^{[l]}||}_F^2$
    - 但，`L2`此时改名为`F`, `Frobenius norm`
  - `L2`的公式：
    - ${||w^{[l]}||}_F^2 = \sum_{i=1}^{n^{[l-1]}}\sum_{j=1}^{n^{[l]}}(w_{ij}^{[l]})^2$, 
    - $w$ 已经不是`vector` 而是 `matrix` 
  - 参数的导数`dw`的公式
    - $dw^{[l]} = (from backpropagation) + \frac{\lambda}{m}w^{[l]}$
  - 化简`w`的更新公式，发现`L1, L2`的实质功效：
    - $w^{{l}} := w^{[l]} - \alpha[(from backprop) + \frac{\lambda}{m}w^{[l]}]$
    - $ = (1-\frac{\alpha\lambda}{m})w^{[l]} - \alpha(from backprop)$
    - 基于 $(1-\frac{\alpha\lambda}{m})$, `L2 or F`被称之为`weight decay`
    - `L1, L2`的效用：压缩模型
      - `lambd`越大，`w`被压缩的越多，模型就越小，
      - 抓取特征能力就越弱，抓取噪音的能力就更弱了
      - <!--反之亦然，但反向操作的情况，不知道有没有人这么做？-->

------

## 如何理解 L2降低overfitting or high variance?

<!--Why regularization reduces overfitting?-->

### L2克服overfitting理解1：压缩模型，控制偏执

![][2.1.10]
- `Cost`: $J(w^{[1]}, b^{[1]}, ..., w^{[L]}, b^{[L]})$
- $=\frac{1}{m}\sum_{i=1}^mL(\hat{y^{(i)}, y^{(i)}})+\frac{\lambda}{2m}\sum_{l=1}^L{||w^{[l]}||}_F^2$
- 要让上述`Cost`最小化，`L2`值也需要最小化，那么
- 如果我们将`lambda`设置比较大，$w^{[l]}$就必须变小，甚至接近0, 并非图中夸张的将部分神经元直接变成0，留下少数神经元不变
- 结果是，一部分`w`都变很小，导致一部分神经元处于休眠状态, 模型缩水，噪音学习减少，`high variance`被下降

### L2克服overfitting理解2：从非线性模型 转化为 线性模型，实现弱化模型学习能力

![][2.1.11]
- 基于上述分析，将`lambda`设置比较大时，$w^{[l]}$就必须变小，甚至接近0
- `linear combination`得到的`z`就越小，从而让`tanh`函数实质上变成了$y=x$函数
- 对应的导数变成1，这样一来，不论有多少层隐藏层，都是linear combination
- 简单的linear函数的预测能力很弱，因此，不会出现学习噪音，导致验证值较差，产生的`high variance`


- 如果使用`L2`,做backward propagation时，一定要用新Cost公式
  - `Cost`: $J(w^{[1]}, b^{[1]}, ..., w^{[L]}, b^{[L]})$
  - $=\frac{1}{m}\sum_{i=1}^mL(\hat{y^{(i)}, y^{(i)}})+\frac{\lambda}{2m}\sum_{l=1}^L{||w^{[l]}||}_F^2$ 
  - $L2$ 是Andrew Ng最常用的`regularization` 方法

------

## dropout抛弃神经元的正则化

### dropout如何降低high variance

- 抛弃神经元，每个样本或批量训练的是一个缩小模型，最后叠加在一起，仍旧是一个缩小版模型，从而弱化模型学习能力；
- 随机抛弃神经元，让任何一个神经元的参数无法过度强化，从而均衡各参数的能力，杜绝出现在某一方面过去偏执

![][2.1.12]
- 每训练一个样本时，随机筛掉50%的神经元（每层输出值中50%的值趋近于0），相当于生成一个小模型
- 每次训练一个新样本，会面对一个新的小模型（保留50%神经元），来进行训练

### Inverted dropout 代码逻辑

![][2.1.13]
- 如果保留80%神经元，`keep_prob=0.8`, 随机筛掉20%
- 对第三层进行上述`dropout`处理，公式如下
  - `a3`: 进入第三层的神经元矩阵
  - 随机筛掉20%的神经元输出
    - `d3=np.random.rand(a3.shape[0],a3.shape[1])<keep_prob`
    - `a3 = np.multiply(a3, d3)`
  - 下面公式的目的：筛掉20%数量，作为补偿，要提高现存输出值的幅度
    - `a3 /= keep_prob`

### 预测不做 dropout 处理

![][2.1.14]

- 测试中，不使用任何dropout
- 也没有任何`scaling` 例如 `/= keep_prob`

------

## 理解dropout克服overfitting

<!--Understanding Dropout-->

关键词：dropout, overfitting, high variance, keep_prob

- 直觉1:

  -  随机丢掉某些层中的神经元，
  -  缩小模型，降低对噪音学习，
  -  从而降低`high variance`

- 直觉2:

  -  以单一输出层模型为例，对输入层做随机`dropout`,
  -  无法完全依靠任何一个特征`feature`，来挑模型预测能力的大梁，
  -  需要将信任平滑到输入层的所有特征值`features`上, 避免出现一个神经元及其参数一家独大
  -  结果接近于：`L2`的`weight decay` ,让所有模型内参数值统一都缩小

- 应用： 下图（右大图), 理论上，对任何一层都可以做`dropout`:
  - 输入层： 通常不希望减少特征值的数量，所以`keep_prob = 1.0`
    - 但图像处理问题`computer vision`，CNN模型经常用`dropout`处理输入层，因为输入层特征值太大，如果模型更深，数据量经常会不够，产生`high bias`
    - 因此，不得不`dropout`处理输入层，让输入层神经元变少，模型深也依然可以应付，也不会要求极端的样本数量
  - 隐藏层：
    - 如果担心该层会出现`overfitting or high variance`，就`keep_prob=0.5`或更小；
    - 如果不是很担心上述问题，可以就`keep_prob = 0.8`左右，甚至`keep_prob=1.0`
  - 实操上：
    - 通常只需要设置一个`keep_prob`值供所有需要`dropout`的层使用，其他层自然无需设置`dropout`或者`keep_prob=1.0`，从而减少寻找最优`keep_prob`的计算量

  ![][2.1.15]

- 弱点代价：
  - 无法直接使用`loss plotting`，来验证模型`gradient descent`是否表现良好，因为训练过程中产生了很多模型，每个模型在`dropout`层，使用不同的`features`特征值来构建模型的预测能力，因此`cost function`无法构建一个平均化的众多模型的总和

- 实操中
  - 先关闭`keep_prob`或者设置`keep_prob=1.0`, 看看`loss plotting`是否正常
  - 如果正常，就可以放心进行`keep_prob`开启状态下的训练了

------

## 其他降低overfitting或underfitting的办法？

### 用`data augmentation`增加`fake data`扩大数据量

- 模型大小深浅不变，更多的数据能帮助模型降低`high variance`，弱化偏执


- `data augmentation`如何工作？对原数据做变形处理（见下图）
  - 旋转， 放大缩小， 翻转， 弯曲变形等
  - 原则上要尽可能模仿真实世界的变形特点和幅度
  - 真实的增量数据肯定优于上述`fake data`，但可能获取成本太高，不现实
  - 如果所有图片猫身体在左边，模型会误认为“身在左边”是猫的重要特征用于预测
  - 让所有样本，拥有各类变形，模型就不会对某一种变形产生偏执与迷恋，帮助模型不纠结各类变形，穿透去学习更深层规律特征上
  - 所以，`data augmentation` 也是一种regularization

![][2.1.16]

- `early stopping`也能降`high variance`
  - 在中间停止训练，虽然`training loss`无法降到最低，但`dev loss`停留在最低值附近
  - 实质：只训练一半，当dev error出现最优值的时候，就停止训练
  - 弱点：没有`orthogalization`, 深度学习独有的优势
    - 先用一组独立的方法，全力让模型overfit
    - 在用一组独立的方法，全力让模型规避overfit, avoid high variance, 或降噪
    - early stopping: 用提前终止训练，找到overfit 和 underfit之间的最优平衡
      - 用同一个方法来平衡overfit and underfit
      - 影响：
        - 不能将overfit and 去high variance做到最好
        - 但执行相对简单
  - L2优劣：
    - 优点：
      - 先做到overfit,
      - 然后让模型用L2训练足够久，就能做到去high variance；
    - 弱点：
      - 需要大量计算
      - 尝试多个值，获取最优的`lambda`值
  - 建议：
    - 如果配置可以，用L2，不用early stopping

![][2.1.17]

------

# 定义你的优化问题 

<!--Set up your optimization problem-->

## 如何标准化输入值？ 

<!--Normalizing Inputs (send today)-->

关键词：`learning_rate`, `input_normalization`, `divergence`, `mean`, `variance`

### 意义： 加速训练

### 实质：规范均值和区间

- 方法1: $x - \mu$, $\mu$ as mean of x, 效果见下图（中）
  - $x1, x2$ 原先均值差异巨大，现在的均值都变成了`0.0`
- 方法2: $\frac{x}{\sigma^2}$, $\sigma^2$ as variance of x
  - $x1, x2$ 的`variance`方差,都变成了`1.0`， 效果见下图（右）
  - 处理前，$x1, x2$ 的分布没有改变，只是分布的区间都在[-1,1]之间了， 效果见下图（中）
- 规范均值为0和方差为1
- 确保：train | dev | test 都得到相同的标准化处理，确保他们的分布相对不变

![][2.1.18]

### 标准化加速训练的实质

![][2.1.19]

- 情景：$x1$ 在`[0,1000]`之间, $x2$ 在`[0,1]`之间
  - 如果不做`input normalization`处理
  - 可能导致： $w1$ 可能处于特别小的区间， $w2$ 处于特别大的区间
  - $J, w1, w2$ 的函数图，多半会如上图（上1）
  - 在更新最优参数，不断探寻损失值最小化的过程中，`learning_rate`学习率不得不变得很小，从而在`w1`的狭小空间中有序推进，否则容易产生`divergence`散度，会远离损失最小值。见上图（下1）
  - 但`learning_rate`很小，在`w2`方向上推进，自然训练很慢
- 情景：如果将上述$x1, x2$做`方法1+方法2`处理
  - 可能导致： $w1$ 和 $w2$ 的空间很均匀规则，比如`[0,1] or [-1,1]`之间
  - $J, w1, w2$ 的函数图，多半会如上图（上2）
  - 在更新最优参数，因为$w1,w2$ 分布均匀，`learning_rate`学习率可以设计大一点，不用担心产生`divergence`散度的问题。见上图（下2）
  - `learning_rate`设置较大，在`W1, W2`方向上，都可以快速推进

  ​

  ​

------

## 梯度消失和梯度爆炸 

<!--Vanishing and Exploding gradients (checked)-->

关键词：gradients, vanishing, exploding, slope

### 实质：初始参数的设定问题

- 很深的模型, 每一层的weights，经过正向或反向传播，会有指数效应
- 如果weight值里有大于1的数，产出预测值时，会出现梯度爆炸
- 反之，梯度消失
- 反向传播时，指数效应会在weights 更新幅度爆炸或消失中体现
- 因此，梯度爆炸时，很难让损失值变小；梯度消失时，损失值变小会特别慢
- 解决方案：在参数初始化上要用心

![][2.1.20]

------

## 模型参数初始化 

<!--Weights initialization for Neural network （checked)-->

关键词：weights_initialization, variance

### 模型初始参数控制在1附近

- 从最简单的例子开始： `4 input neurons, 1 output neuron` 见下图（左上部）
  - 目的：让`z`不要太大或太小
    - $z = w_1x_1 + w_2x_2 + ... + w_nx_n + (b)$ 这里姑且忽略`b`
    - 让$z$不能过大过小
      - 如果`n`层数很多，那么`w`值就需要够小
  - 解决方案：
    - 让每一层的`w`不要比`1`大太多，也不要比`1`小太多
    - 这么做不能消除梯度爆炸和消失，但可以缓解病情
    - <!--为什么只是部分解决？因为指数关系还在？-->

### 寻找W和n之间的关系函数

- 如何将`w`设置在合适的大小？
  - $\sigma(w) = \frac{1}{n}$ 用`n`来限定`w's variance`
  - $W_{rand}^{[l]} = np.random.rand(W.shape)$
  - $C_{ctrl} = np.sqrt(\frac{1}{n^{[l-1]}})$
    - 如果`g(z)=relu(z)`, use $\frac{2}{n^{[l-1]}}$
    - 如果`g(z) = tanh(z)`, use $\frac{1}{n^{[l-1]}}$
    - 还有的paper, 推荐 $\frac{2}{n^{[l-1]+n^{[l]}}}$
    - 可以在如何定义更合适的 $\sigma$ `variance` 上下功夫
    - 也可以设置一个次级高阶参数(作为`varance`的乘数)，在训练中找到最佳的该参数值
  - $W^{[l]} = W_{rand}^{[l]}C_{ctrl}$

![][2.1.21]

------

## gradient checking导数验证

<!--Numerical Approximation of Gradients-->

关键词：gradient_checking, backprop, slope, epsilon, order_O

### 验证backprop

### 计算精度高但费时

- 应用：手动计算`slope`，用2倍的 $\epsilon$ 来计算`slope`，验证模型代码提供的自动`slope`值
  - 公式： $f^\prime(\theta)=\lim_{\epsilon\to0}\frac{f(\theta+\epsilon)-f(\theta-\epsilon)}{2\epsilon}$, 
  - 不是 $f^\prime(\theta)=\lim_{\epsilon\to0}\frac{f(\theta+\epsilon)-f(\theta)}{\epsilon}$
  - 好处：
    - 当 $\epsilon$ 为0.01
    - 上述方法产生的误差幅度是$O(\epsilon^2)$, 误差幅度为0.0001倍
    - 用1倍的 $\epsilon$ 来计算`slope`，误差幅度是$O(\epsilon)$， 误差幅度为0.01倍
    - 因此精准度更高，是优点；但计算量比单倍计算方法要慢
    - $$f^\prime(\theta)=\lim_{\epsilon\to0}\frac{f(\theta+\epsilon)-f(\theta-\epsilon)}{2\epsilon}$$

![][2.1.22]

------

## 模型训练中做gradient checking 

<!--Gradient Checking-->

### why - 寻找模型backprop中的错误

### 将所有的parameters排成一列

![][2.1.23]

求出gradient checking值，与gradient值做比较:

- 如果误差只有$10^{-7}$, 说明**gradient**算的没问题
- 如果误差只有$10^{-5}$, 说明**gradient**可能没问题，需要检查代码
- 如果误差只有$10^{-3}$, 说明**gradient**肯定有问题

![][2.1.24]

------

## Gradient Checking细节问题 

<!--Gradient CheckingImplementation Notes-->

![][2.1.25]

- 在训练中不要使用这种方法;
- 如果计算有错误, 查看**dw, db**组成部分的
- 要记住规范化的损失函数公式，会更复杂
- 对于使用抛弃层的模型,要先关闭抛弃层然后再计算
- 对比:现在随机初始化的时候计算, 然后在训练几次之后再算一遍。

------

[2.1.01]: http://r.photo.store.qq.com/psb?/V119hAgO09pOsG/z8HP4oD5SAV89gxwOCG4cbxwLspc2X.p7ZO3MF7H42k!/r/dG0BAAAAAAAA
[2.1.02]: http://r.photo.store.qq.com/psb?/V119hAgO09pOsG/LCCZew.91fMF73CtzQxCKD.r6jKX0fGK2Z4uT69NIds!/r/dDwBAAAAAAAA
[2.1.03]: http://r.photo.store.qq.com/psb?/V119hAgO09pOsG/55URpQre7r6vLaahPCKmUiKm1xoh0aHIAk*t1FRCO5o!/r/dDwBAAAAAAAA
[2.1.04]: http://r.photo.store.qq.com/psb?/V119hAgO09pOsG/0nssAcWJVkRYIJRWHkEJE9MO0dZ7vjTCzWWZ4h3QUhM!/r/dDwBAAAAAAAA
[2.1.05]: http://r.photo.store.qq.com/psb?/V119hAgO09pOsG/CG0Te*wp..Z6UzCig*F6HgO*7UzTh8Y7yhoTxac*.rc!/r/dG4BAAAAAAAA
[2.1.06]: http://r.photo.store.qq.com/psb?/V119hAgO09pOsG/ZR*VRd7luSrgtJhfGxv5r49oGpre7N8K3uuZftrmrbM!/r/dD0BAAAAAAAA
[2.1.07]: http://r.photo.store.qq.com/psb?/V119hAgO09pOsG/m4rllDdtsXYL4QjjIMkzgHZHwcKB4gtfTNftrOjC6sU!/r/dDwBAAAAAAAA
[2.1.08]: http://r.photo.store.qq.com/psb?/V119hAgO09pOsG/K2CcY6tTvoh8I8QR0AhwVS0jQLLBOtbupKz3Wvj*X8E!/r/dD8BAAAAAAAA
[2.1.09]: http://r.photo.store.qq.com/psb?/V119hAgO09pOsG/J8zalCBmyDmZNOVl9g*PUNYQEEqtlCPIdefhXzgBwxo!/r/dD0BAAAAAAAA
[2.1.10]: http://r.photo.store.qq.com/psb?/V119hAgO09pOsG/gDQseXyDv0U830O1VAJYAYXDgH7BSQKmVYdwpM6PlcE!/r/dDwBAAAAAAAA
[2.1.11]: http://r.photo.store.qq.com/psb?/V119hAgO09pOsG/Hz0ty7g9ypy*YzmRcekAgW6dg2RDTuljTMlgmXhmSRQ!/r/dD0BAAAAAAAA
[2.1.12]: http://r.photo.store.qq.com/psb?/V119hAgO09pOsG/5ZT2HljMgtsc.u1q4w1vVdnjTDwC46AGaYUN3nuMHsc!/r/dGwBAAAAAAAA
[2.1.13]: http://r.photo.store.qq.com/psb?/V119hAgO09pOsG/3mOYD2oDblWR03s9rjdIj1RRaRYMz*jHCHhVk9qL5XE!/r/dG0BAAAAAAAA
[2.1.14]: http://r.photo.store.qq.com/psb?/V119hAgO09pOsG/WmE2NPrMZI5tZsNbwAIr74xR3.4m4eWtUsxqZXvHxF0!/r/dGwBAAAAAAAA
[2.1.15]: http://r.photo.store.qq.com/psb?/V119hAgO09pOsG/hrpNEHNCuPok4trljjjX2L9riq1jQyvJNZ*OBeTf8UE!/r/dGwBAAAAAAAA
[2.1.16]: http://r.photo.store.qq.com/psb?/V119hAgO09pOsG/V9bep7BvbQTi0a2Df0j7zzIqyCVuRd3HgiL32qO3tB4!/r/dD0BAAAAAAAA
[2.1.17]: http://r.photo.store.qq.com/psb?/V119hAgO09pOsG/qjh9PaKYjPXTBAGHv07MUIBrVSbONdeA6LfTCeUvjgU!/r/dDwBAAAAAAAA
[2.1.18]: http://r.photo.store.qq.com/psb?/V119hAgO09pOsG/lg*lhD*LGJqY4WcudzF5DOlE68HjpeWPB.KzZBcdGWw!/r/dPcAAAAAAAAA
[2.1.19]: http://r.photo.store.qq.com/psb?/V119hAgO09pOsG/zPrq4djwT1b6yJDcxDOvT.dBullba4ebBfRdcPCak68!/r/dDwBAAAAAAAA
[2.1.20]: http://r.photo.store.qq.com/psb?/V119hAgO09pOsG/cdnnXQnfSDfuK8t7NN90HFAEfxYUutPEQAxQeyLlaOQ!/r/dGwBAAAAAAAA
[2.1.21]: http://r.photo.store.qq.com/psb?/V119hAgO09pOsG/nPSTrKf2lpAhS7aBAu.kurAKoXNvPDhDjEfz0ik38RU!/r/dDwBAAAAAAAA
[2.1.22]: http://r.photo.store.qq.com/psb?/V119hAgO09pOsG/hJLLJLoiZHj08jMBFZVkj6st9Ab*O3N30ZUaNGW32hw!/r/dG0BAAAAAAAA
[2.1.23]: http://r.photo.store.qq.com/psb?/V119hAgO09pOsG/LwJL1JAY*aO3R3MellimhrSJarA8nKnvlJAc.DhF50M!/r/dD0BAAAAAAAA
[2.1.24]: http://r.photo.store.qq.com/psb?/V119hAgO09pOsG/d4Pon1MPjwqhsc0jzSbw.yy6J0ARLJr.fCVZxP.MnTA!/r/dG4BAAAAAAAA
[2.1.25]: http://r.photo.store.qq.com/psb?/V119hAgO09pOsG/KjLIU6utOUpYYWoFt7E3frmKzvLEEHI4CxKK.dNdQKA!/r/dG0BAAAAAAAA


# 第二周 优化算法

<!--Optimization Algorithms-->

## Learning Objectives

- 几个常用的学习算法 optimization methods such as **(Stochastic) Gradient Descent**, **Momentum**, **RMSProp** and **Adam**
  - <u>什么是学习算法</u>: 优化过的快速更新参数的算法
- 用随机小批量样本，加速学习和改进优化 Use random minibatches to accelerate the convergence and improve the optimization
  - <u>应用痛点</u>：快速生成训练结果，因为将全部样本训练分割成众多小样本训练
- 学习率的消减和在学习算法中的应用 the benefits of learning rate decay and apply it to your optimization
  - <u>应用痛点</u>：不用费劲尝试最佳learning_rate

------

## mini-batch gradient descent

### 意义

- 深度学习，是实验科学，需要反复实验，不停修正
- 要求快速训练，得到验证结果 
  - 但1000000样本，想看损失值，全样本训练一次，太慢太耗时
  - 我们要频繁重复：实验，修改超参数，再实验
  - 关键是要能快速了解损失值**<u>动态变化细节</u>**， 怎么办？
  - 如果分割成无数个32， 64或 128为一组（一批）的小样本集合，一个集合出一次损失值
  - 这样短时间，看到多个损失值，看到动态变化轨迹
- mini-batch gradient descent 帮助**<u>加速实验循环速度，展示动态变化细节</u>**

### Batch vs Stochastic vs Mini-batch gradient descent

 - **Vectorization**: 加速多样本计算，无需loop，提高效率
 - Batch gradient descent 是对全部样本做backward prop
    - 比如，5000000样本，vectorization能高效训练 <u>(forward, backward)</u> 得到最终损失值，
    - 但出一次损失值，耗时太久
 - stochastic gradient descent 针对单一样本做backward prop
    - 很快看见损失值，因为只算一个样本
    - 但Vectorization用不上，整体训练太低效
 - mini-batch gradient descent 是对小份样本(mini-batch)做backward prop
    - 如果将全部样本，切割成5000份，1000/份
    - 出一次损失值，只需计算一个小样本组，等待时间可接受
    - 每个一小份样本组内部做 <u>(forward, backward)</u> ，用vectorization做一次得到损失值，训练效率没有明显丢失

![][2.2.00]

### 符号

- $X$ 总样本维度（n, 5000000)
- $x^{(i)}$ 单个样本
- $z^{[l]}$ 第L层的线性处理值
- $X^{\{1\}}$ 第一个小批量样本

![][2.2.01]

### mini-batch gradient descent 执行代码

![][2.2.02]

------

## Understanding mini-batch gradient descent

### 损失图对比

![][2.2.03]

### Batch vs Stochastic vs mini-batch 损失值图比较

Batch gradient descent: 虽然整套训练较快（使用了vectorization), 但每次损失值更新很慢（需要计算所有样本）

![][2.2.04]



### 如何选择mini_batch_size ?

- 什么情景下用mini_batch?
- mini_batch_size应该多大？
- 不要超出你的cpu|gpu 内存

![][2.2.05]

## Exponentially Weighted Averages

### 意义

- 帮助理解其他优于gradient descent 算法
- 是理解其他算法的基础 <!--Andrew Ng was born in London-->

### 伦敦10日气温均线

![][2.2.06]

- exponential weighted average 公式及理解

![][2.2.07]

------

## Understanding Exponential Weighted Average

### Exponential是如何而来？

![][2.2.08]

### 天数周期计算公式是怎么来的

![][2.2.09]

### 为什么选用Exponential Weighted Average 求移动均线值

![][2.2.10]



## Bias Correction in Exponential Weighted Average

### 存在价值

- 让exponential weighted average 计算更加准确
- 哪里存在不准确
  - 第3步，化简后的公式，解释在早期，
  - $\theta_1=40, \theta_2=10$ 时，
  - $V_{实际1}=\frac{40}{1}, V_{实际2}=\frac{40+10}{2}$， 
  - 但是 $V_1=8, V_2 = 8.04$ 
  - 这就是绿线和紫线差距的由来
- 如何纠正
- 直观数学原理

![][2.2.11]

### Bias Correct 不是必须

- 并不是所有人都在意早期阶段的bias问题

------

## Gradient Descent with Momentum

### 一句话

- Gradient descent + exponential weighted average = momentum 比标准的gradient descent 表现更好

### mini-batch gradient descent 的运动状态中的矛盾

### 加权平均如何能解决这种运动矛盾

![][2.2.12]

### momentum 代码逻辑

![][2.2.13]

## RMSProp

<!--root mean squre Prop-->

### 与Momentum相似点

- 应对相似gradient descent 运动形态，纵轴波动大方向相反，横轴方向一致
- 都用指数权重平均值算法

### 差异点

- Momentum 只单纯用指数权重平均值来更新参数
- RMSProp用指数权重平均值的sqrt来更新参数

![][2.2.14]

### 希望如何变现实

- 要实现gradient descent 形态改进为绿线，我们希望$dw^2$ 变小， $db^2$ 变大
- 为什么能如愿？
  - mini-batch gradient descent 蓝线特征是现实常规
  - 损失函数，约定要想尽一切办法尽快降低损失值，逼近最小值，这是事先设定的
  - Momentum and RMSProp是帮助损失函数克服蓝线的现实，尽快实现目的
  - 通过RMSProp的公式设定，在反向传播backprop中，会倒逼$dw^2$ 变小， $db^2$ 变大

## Adam

<!--Adaptive moment estimation-->

### 核心优势

### 公式解析

### 可调参数

![][2.2.15]

![][2.2.16]

## 学习率衰减

<!--Learning rate decay-->

### 效用

- 逐步削减`learning rate`来实现加速学习

### 原理

- mini-batch: 64, 128, 波动性大
- 固定learning_rate
  - 如果一开始设定较大， 前期速度快，后期难逼近最小值；
  - 如果一开始设定较小，最终能逼近最小值，但速度太慢；
- 对learning_rate的希望：一开始快，然后逐步削减

![][2.2.17]

### 代码逻辑

![][2.2.18]

### 其他学习率消减的方法

![][2.2.19]

### 使用注意

- 一级参数： `learning_rate`; 二级参数： `decay_rate`
- `hyperparameter tuning` 能帮助我们系统调试最优参数

## 局部最优的问题

<!--problem of local optimal-->

### 模型训练的目的

- 寻找最小损失值， $J_{min}$

### 如何从图形上理解$J_{min}$

- $J, w_1$, $J, w_2$ 都形成U型函数图
- 合在一起，形成类似碗形的图
- optimal 就是碗的最低点， J的最小值

![][2.2.20]

### local optimal痛苦之处

- $J, w_1, w_2$创造了不止一个碗, 有无数中高位置的碗`local optimal`
- 当训练进入这些碗后，在他们的最低点，损失值会变0， 训练停止
- 我们要的是最深最低的一个碗的最低点 `global optimal`

### 从local optimal到saddle point

- local optimal 是浅层模型特有的，双U型或碗形概率25%，不低；但对深层模型，概率$10^{-20000}$
- saddle point 是U型和倒U型结合，概率就高很多很多，非常常见
- saddle point 带来的问题，不是slope=0, 停止训练；而是有些地方训练很慢很慢

![][2.2.20]

### 优化算法的角色

![][2.2.21]



# 第三周 超参数调试、Batch正则化和程序框架

<!--Hyperparameter tuning, Batch Normalization and Programming Frameworks-->

## 调试处理

<!--Tuning process-->

### 超参数的优先度分级

![][2.3.01]

### 超参数组选取方法

- 用随机，不用固定模式组合
- 聚焦再搜索

![][2.3.02]

![][2.3.03]



## 选择合适的尺度scale来调试超参数

### uniform random随机选择的情况

![][2.3.04]

### log scale 随机选择

![][2.3.05]

### 权重平均值为例

![][2.3.06]

## 超参数训练实战经验

### 不同领域模型的经验与参数借鉴度

- 思路，经验，技巧在不同领域可借鉴
- 最佳参数值，通常无法借用
- 同一个模型领域，一两个月，需要重新训练验证最优参数值是否发生变化

![][2.3.07]

### 常见超参数验证（训练）方法

- 熊猫： 专注训练一个模型，实时关注，根据需要调整超参数，不断优化
  - 适用人群：数据量大，计算能力有限
  - <!--调试哪些超参数不会让模型本质发生变化？-->
  - 至少learning_rate $\alpha$, momentum $\beta$ , Ng提及可以自由调试
- 鱼卵：同时训练多个模型（不同超参数提前设定好），最后对比，找到最优超参数值
  - 适用人群： 计算能力特别强大
  - 多个模型的区别：
    - 超参数的区别（相同模型，不同超参数）
    - 模型本质区别（不同模型，不同超参数）

![][2.3.08]

## batch normalization

### 效用

- 帮助简单高效稳定地寻找最优超参数值
- 让超参数在更大的区间，有效服务你的模型
- 让更深更大的模型训练起来更容易

### Batch Norm实际工作

- 广泛的输入值标准化
  - 对输入层做平均值，方差处理
  - 对隐藏层做相同处理

![][2.3.09]

### Batch Norm 代码逻辑

- 建议对$z$而不是 $a$ 做处理
- 通过 $\gamma$ 和 $beta$ 来控制 $z$ 的平均值和方差，从而控制 $z$ 的变化范围
- <!--疑问--> 学习算法，gradient descents 也会对$\gamma, \beta$
  进行更新吗？还是它们完全有人来设定？

![][2.3.10]



## batch norm 应用于神经网络模型

<!--fitting batch norm in a neural network-->

### $\gamma, \beta$ 可训练更新参数

![][2.3.11]

### $b$ 可忽略

![][2.3.12]

### $\gamma, \beta $ 像$w, b$ 被优化算法更新

![][2.3.13]

## batch norm能加速学习的深层原因

<!--why batch normalization work?-->

### Covariance shift

![][2.3.14]

### batch norm 克服 covariance shift

![][2.3.15]

### 有限regularization

![][2.3.16]



## Batch norm at test time

### 预测时没有mini-batch的使用，如何求均值与方差？

在训练中，对所有层的均值方差的不同样本下的值，做指数加权平均，最后的均值和方差，用在test的模型里来计算batch norm. 

![][2.3.17]



# 多类别分类

## softmax regression

### 理解softmax

![][2.3.18]

### 浅层线性分类模型图

![][2.3.19]



## 训练一个softmax分类模型

### 进一步理解softmax

![][2.3.20]

### 基于softmax的损失函数

![][2.3.21]

### softmax分类的反向传播

![][2.3.22]

# 介绍深度学习库

## 如何选择深度学习库

![][2.3.23]

## 如何使用tensorflow

### 求解损失函数的最小值对应的w

![][2.3.24]

### 如何带入数据

![][2.3.25]

### 反向自动化

![][2.3.26]







------



[2.2.01]:http://r.photo.store.qq.com/psb?/V119hAgO09pOsG/soXWEFDVGJRUV1XE.KDWxwxnayaMUWNgH3.srcAn4lk!/r/dOcAAAAAAAAA
[2.2.00]:http://r.photo.store.qq.com/psb?/V119hAgO09pOsG/t3eEl55kBg3UC9g8tVFthDHkEgVIvOz**woWN8glS40!/r/dD0BAAAAAAAA
[2.2.02]:http://r.photo.store.qq.com/psb?/V119hAgO09pOsG/82CDcBfLETbcKfGurhSABP2itdXdNYQTYgDnPTXCc.0!/r/dG0BAAAAAAAA
[2.2.03]:http://r.photo.store.qq.com/psb?/V119hAgO09pOsG/*USiJNyO41cp9Zb9CvA7TZX6dCwufrtlEhTtFfA6vWU!/r/dGwBAAAAAAAA
[2.2.04]:http://r.photo.store.qq.com/psb?/V119hAgO09pOsG/tZwi2y8A114B4O0p1LzD9*y25mGSTrl29wz9.T6t0KQ!/r/dDwBAAAAAAAA
[2.2.05]:http://r.photo.store.qq.com/psb?/V119hAgO09pOsG/SZXNeXztWXZ8GXfvk4NHV9GtHBy.eHdK4BjaD*zhzOw!/r/dD0BAAAAAAAA
[2.2.06]:http://r.photo.store.qq.com/psb?/V119hAgO09pOsG/w48Uuj*0hiJG4hjpVoQw04Qo5MAjexJVmfH2EeDZXtw!/r/dD0BAAAAAAAA
[2.2.07]:http://s316.photo.store.qq.com/psb?/V119hAgO09pOsG/bf1VGu06NMB3PNTWdBtsJxA*HIXblRcPHYmYx5AZPG4!/r/dDwBAAAAAAAA
[2.2.08]:http://r.photo.store.qq.com/psb?/V119hAgO09pOsG/7LB*oPb9hJiGU8GLPLiHNQzbiVsXXbqAeYOYP20Cx4Y!/r/dDwBAAAAAAAA
[2.2.09]:http://r.photo.store.qq.com/psb?/V119hAgO09pOsG/P9KNx5Jn4YIQEme6GpHZAhobvy1qWtaENIytUKEKCXw!/r/dGwBAAAAAAAA
[2.2.10]:http://r.photo.store.qq.com/psb?/V119hAgO09pOsG/4SkVU2K6W401gtslZYaQeX73eGicLNjcwu7BHXHDARI!/r/dD0BAAAAAAAA
[2.2.11]:http://r.photo.store.qq.com/psb?/V119hAgO09pOsG/nZeX8N6QZnmQicJxPq0yPd*qHMBU2XPpP3yfumxXkVg!/r/dG0BAAAAAAAA
[2.2.12]:http://r.photo.store.qq.com/psb?/V119hAgO09pOsG/J2625o3o5DZobCyFzfzYZaMn2Lws8vOxUyyH5SqO9rI!/r/dG0BAAAAAAAA
[2.2.13]:http://r.photo.store.qq.com/psb?/V119hAgO09pOsG/2u1Xz.scXW9S.efommtoCO.SYquVsDDcXyPew5M188s!/r/dDwBAAAAAAAA
[2.2.14]:http://r.photo.store.qq.com/psb?/V119hAgO09pOsG/0eWVjfGB3KegAcoN9LlheAQ7maall2pn5FyB0VVXmKA!/r/dD0BAAAAAAAA
[2.2.15]:http://r.photo.store.qq.com/psb?/V119hAgO09pOsG/SujRNGROa1ghush8sR55d47jTgK3ZDfzLmIGSqOA2Ic!/r/dDwBAAAAAAAA
[2.2.16]:http://r.photo.store.qq.com/psb?/V119hAgO09pOsG/X0184mbYQTfv2dnuMrEuMoEOVcW2bOwMQ5KYIn45bRo!/r/dD0BAAAAAAAA
[2.2.17]:http://r.photo.store.qq.com/psb?/V119hAgO09pOsG/vKqlNcn0GnyS2xc*NniJ8cyrIfZJI1YklivK*M*.6x4!/r/dDwBAAAAAAAA
[2.2.18]:http://r.photo.store.qq.com/psb?/V119hAgO09pOsG/jw4V82q20WhY5BeTRRi21VJlbdYIMiBJ9J0hgAjGyck!/r/dD0BAAAAAAAA
[2.2.19]:http://r.photo.store.qq.com/psb?/V119hAgO09pOsG/SKG1MzrqmzXmXVwyXG5qI1nGUFX..gE9G0aNbrj05rk!/r/dD0BAAAAAAAA
[2.2.20]:http://r.photo.store.qq.com/psb?/V119hAgO09pOsG/qunHso8vzke2Gp2s2skcvvonERY.INN5DmynqJdqHnI!/r/dG0BAAAAAAAA
[2.2.21]:http://r.photo.store.qq.com/psb?/V119hAgO09pOsG/dUDarkLImE07S4EnXOGRJmNOyk8UDPFuF5ruRGH.zHw!/r/dG0BAAAAAAAA
[2.3.01]:http://r.photo.store.qq.com/psb?/V119hAgO09pOsG/zTSKdkXUvldCWMgOry3Y0QstqMd.gitZmTkN.AMDYmY!/r/dD0BAAAAAAAA
[2.3.02]:http://r.photo.store.qq.com/psb?/V119hAgO09pOsG/Wcfm7AZzeb8Ap2DlUE8DIz.C6VcI0TFyyW03KE77sns!/r/dD0BAAAAAAAA
[2.3.03]:http://r.photo.store.qq.com/psb?/V119hAgO09pOsG/PXVyBBr1C*hX5d2zpP5226Sm4sKflHBQ5Yo64NtkuD4!/r/dDwBAAAAAAAA
[2.3.04]:http://r.photo.store.qq.com/psb?/V119hAgO09pOsG/Nq49dPVCg1IDSKsDCf7PJ0rKkm4i8OLkelBMNsMs7S8!/r/dGwBAAAAAAAA
[2.3.05]:http://r.photo.store.qq.com/psb?/V119hAgO09pOsG/uj*hD4RWocSUn7CWFK9K8pA31oqgO1DQ7MLqNMitQ0M!/r/dD0BAAAAAAAA
[2.3.06]:http://r.photo.store.qq.com/psb?/V119hAgO09pOsG/fzIMZCykAOCr3fmFHClskHBSB5QUgAtioXYB1Q4Eh2M!/r/dDwBAAAAAAAA
[2.3.07]:http://r.photo.store.qq.com/psb?/V119hAgO09pOsG/37f*u3QJ686BViGR.T9QiZNwKfs0PmuHFFUxOXxlb3c!/r/dDwBAAAAAAAA
[2.3.08]:http://r.photo.store.qq.com/psb?/V119hAgO09pOsG/iqkJZljMhUfrT4y.wSdou277Ug4Enn5UQvBax7MQTF4!/r/dG4BAAAAAAAA
[2.3.09]:http://r.photo.store.qq.com/psb?/V119hAgO09pOsG/cSx*M41iRzQDoRxL69pOyVR04PlysCAnh7Wjl7iPIwI!/r/dIUBAAAAAAAA
[2.3.10]:http://r.photo.store.qq.com/psb?/V119hAgO09pOsG/KqpLrxD9Supa32*za4HieAsaW4KQCoshcJtmqyGZ5rg!/r/dDwBAAAAAAAA
[2.3.11]:http://r.photo.store.qq.com/psb?/V119hAgO09pOsG/uOSPstc.NY1evohCiQHl4ctsK4JJL2IWsnAJYEY0IUA!/r/dOcAAAAAAAAA
[2.3.12]:http://r.photo.store.qq.com/psb?/V119hAgO09pOsG/Iwl5kYovJ5phnAhcGmFcvkEHDJkCS9pwb1JdijEgrw0!/r/dDwBAAAAAAAA
[2.3.13]:http://r.photo.store.qq.com/psb?/V119hAgO09pOsG/BLPlNhYLNG7fkpPTUQfqWHkLIgxE9gX1B7ueO5vaGCA!/r/dD0BAAAAAAAA
[2.3.14]:http://r.photo.store.qq.com/psb?/V119hAgO09pOsG/cKVc9OOLwBDdyNw9bOh*mksMQjPHPyeImlpG6ZRPviw!/r/dIUBAAAAAAAA
[2.3.15]:http://r.photo.store.qq.com/psb?/V119hAgO09pOsG/.cYFCrVdPKgHpCvsi2zSToNWMz6NjoU.IQXl7nv.ct0!/r/dIUBAAAAAAAA
[2.3.16]:http://r.photo.store.qq.com/psb?/V119hAgO09pOsG/hQ3hYDenBAC0XxkpiL5sxz9TKVajaKL64dGtcC*ePgk!/r/dDwBAAAAAAAA
[2.3.17]:http://r.photo.store.qq.com/psb?/V119hAgO09pOsG/GO8XEa2g2aVGY7LyCIGYTd9gzxA8sa7EOW1zt22*Ymg!/r/dGwBAAAAAAAA
[2.3.18]:http://r.photo.store.qq.com/psb?/V119hAgO09pOsG/3kKXMVqs0G5z2K3cu5uF981GgCTnfb22z651D96p9dE!/r/dG0BAAAAAAAA
[2.3.19]:http://r.photo.store.qq.com/psb?/V119hAgO09pOsG/G7jx24gqB.ZH8vHKnDm1Qla6y9Ifa7BqS3ZnpPHfIAM!/r/dDwBAAAAAAAA
[2.3.20]:http://r.photo.store.qq.com/psb?/V119hAgO09pOsG/Vw7KIThNcZ0Pfzeld3k6mFlkuFFDl8OkkSdbUZDvb.4!/r/dD0BAAAAAAAA
[2.3.21]:http://r.photo.store.qq.com/psb?/V119hAgO09pOsG/iieHwQpqvP3Bma6WCfa0gWnygeo85Kc*LxAkyhnzKIA!/r/dDwBAAAAAAAA
[2.3.22]:http://r.photo.store.qq.com/psb?/V119hAgO09pOsG/nDh9MEgb3MeqZd0Bv*yFvLAdZzxaVWhRvlwpBGAqElA!/r/dIUBAAAAAAAA
[2.3.23]:http://r.photo.store.qq.com/psb?/V119hAgO09pOsG/PbQcBYbIeOGflB1l0Tci42Dv.IlRl7oYo55St8VvqHU!/r/dOcAAAAAAAAA
[2.3.24]:http://r.photo.store.qq.com/psb?/V119hAgO09pOsG/2xTnL.CBoIU6sNVjxYP*CJt.xk72nrNPhcSgC36OUsM!/r/dIUBAAAAAAAA
[2.3.25]:http://r.photo.store.qq.com/psb?/V119hAgO09pOsG/njwzcwkmgt4qFG7n8W7gcRZ9MAsEki4F.qxjVHz.fp4!/r/dIUBAAAAAAAA
[2.3.26]:http://r.photo.store.qq.com/psb?/V119hAgO09pOsG/yIpCvatK0TPaS3Jct72LE5iBAZdQBBH55a8npfT1zfY!/r/dIUBAAAAAAAA